[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Reasoning",
    "section": "",
    "text": "Introduction and Reader’s Guide\nIn the years that I have been teaching inferential statistics to bachelor students in Communication Science, I have learned two things. First, it is paramount that students thoroughly understand the principles of statistical inference before they can apply statistical inference correctly themselves. Second, formal notation, manual calculation, and estimation details distract rather than help students understand what they are doing. This book offers a non-technical but thorough introduction to statistical inference. It discusses a minimal set of concepts needed to understand both the possibilities and pitfalls of estimation, null hypothesis testing, moderation, and mediation analysis. It uses a minimum of formal notation."
  },
  {
    "objectID": "index.html#intended-audience-and-setting",
    "href": "index.html#intended-audience-and-setting",
    "title": "Statistical Reasoning",
    "section": "Intended Audience and Setting",
    "text": "Intended Audience and Setting\nThis book is written as reading material for a follow-up course in statistics, in the bachelor of Communication Science at the University of Amsterdam. Students enrolled in this course have passe an introductory course in statistics that explained how to change research questions into variables and associations between variables, how to select and execute the correct analysis or test (in SPSS) to answer their research question, and how to interpret the results in a language that is both comprehensible for the average reader and complying with professional standards (APA standard for reporting test results). In addition, they have learned the very basics of inferential statistics: How to decide which null hypothesis to reject based on reported p values, and how to interpret confidence intervals.\nThis book is meant for use in a flipped-classroom setting. Students should read the text, watch embedded videos, and play with the interactive content before they meet in class. Class meetings are used to answer questions raised by the students, do group work to exercise with the concepts and techniques presented in the text, and do short tests to check understanding."
  },
  {
    "objectID": "index.html#interactive-content",
    "href": "index.html#interactive-content",
    "title": "Statistical Reasoning",
    "section": "Interactive Content",
    "text": "Interactive Content\nThe interactive content in this book replaces simulations that used to be demonstrated during lectures. I expect that doing simulations yourself rather than watching them being done by someone else enhances understanding. I have tried to break down the simulations into smaller steps, confronting the student several times with essentially the same simulation, but with added complexity. I hope that this approach enhances understanding and remembrance and, at the same time, avoids frustration caused by complex dashboards offering all options at once."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Statistical Reasoning",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe example data sets have been generated for the purpose of demonstrating statistical techniques. These are not real data and no conclusions should be drawn from the results obtained from the data."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Reasoning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nAdam Sasiadek developed the more complicated Shiny apps in this book. The College of Communication at the University of Amsterdam generously supported the creation of the apps whereas this university’s Grassroots Project for ICT in education refused to support it. Renske van Bronswijk corrected my English. Any remaining errors result from changes and additions that I applied afterwards. My colleague Peter Neijens commented on a draft of this text. Among the first tutors using this book, Chei Billedo, Marcel van Egmond, Matthijs Elenbaas, Andreas Goldberg, Bregje van Groningen, Luzia Helfer, Rhianne Hoek, Laura Jacobs, Jeroen Jonkman, Fam te Poel, Sanne Schinkel, Christin Scholz, Ragnheiður Torfadóttir, Philipp Mendoza, and she whose name I am not allowed to mention offered many suggestions that improved the text substantially. Among the students who helped to improve the book, Alissa Hilbertz stands out for her careful language corrections and suggestions.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "01-samplingdistr.html#statistical-inference-making-the-most-of-your-data",
    "href": "01-samplingdistr.html#statistical-inference-making-the-most-of-your-data",
    "title": "1  Sampling Distribution: How Different Could My Sample Have Been?",
    "section": "1.1 Statistical Inference: Making the Most of Your Data",
    "text": "1.1 Statistical Inference: Making the Most of Your Data\nStatistics is a tool for scientific research. It offers a range of techniques to check whether statements about the observable world are supported by data collected from that world. Scientific theories strive for general statements, that is, statements that apply to many situations. Checking these statements requires lots of data covering all situations addressed by theory.\nCollecting data, however, is expensive, so we would like to collect as little data as possible and still be able to draw conclusions about a much larger set. The cost and time involved in collecting large sets of data are also relevant to applied research, such as market research. In this context we also like to collect as little data as necessary.\nInferential statistics offers techniques for making statements about a larger set of observations from data collected for a smaller set of observations. The large set of observations about which we want to make a statement is called the population. The smaller set is called a sample. We want to generalize a statement about the sample to a statement about the population from which the sample was drawn.\nTraditionally, statistical inference is generalization from the data collected in a random sample to the population from which the sample was drawn. This approach is the focus of the present book because it is currently the most widely used type of statistical inference in the social sciences. We will, however, point out other approaches in Chapter @ref(crit-discus).\nStatistical inference is conceptually complicated and for that reason quite often used incorrectly. We will therefore spend quite some time on the principles of statistical inference. Good understanding of the principles should help you to recognize and avoid incorrect use of statistical inference. In addition, it should help you to understand the controversies surrounding statistical inference and developments in the practice of applying statistical inference that are taking place. Investing time and energy in fully understanding the principles of statistical inference really pays off later."
  },
  {
    "objectID": "01-samplingdistr.html#discreterandomvariable",
    "href": "01-samplingdistr.html#discreterandomvariable",
    "title": "1  Sampling Distribution: How Different Could My Sample Have Been?",
    "section": "1.2 A Discrete Random Variable: How Many Yellow Candies in My Bag?",
    "text": "1.2 A Discrete Random Variable: How Many Yellow Candies in My Bag?\nAn obvious but key insight in statistical inference is this: If we draw random samples from the same population, we are likely to obtain different samples. No two random samples from the same population need to be identical, even though they can be identical.\n\nSample statistic\nWe are usually interested in a particular characteristic of the sample rather than in the exact nature of each observation within the sample. For instance, I happen to be very fond of yellow candies. If I buy a bag of candies, my first impulse is to tear the bag open and count the number of yellow candies. Am I lucky today? Does my bag contain a lot of yellow candies?\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\nThe number of yellow candies in a bag is an example of a sample statistic: a value describing a characteristic of the sample. Each bag, that is, each sample, has one outcome score on the sample statistic. For instance, one bag contains four yellow candies, another bag contains seven, and so on. All possible outcome scores constitute the sampling space. A bag of ten candies may contain 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, or 10 yellow candies. The numbers 0 to 10 are the sampling space of the sample statistic number of yellow candies in a bag.\nThe sample statistic is called a random variable. It is a variable because different samples can have different scores. The value of a variable may vary from sample to sample. It is a random variable because the score depends on chance, namely the chance that a particular sample is drawn.\n\n\nSampling distribution\nSome sample statistic outcomes occur more often than other outcomes. We can see this if we draw very many random samples from a population and collect the frequencies of all outcome scores in a table or chart. We call the distribution of the outcome scores of very many samples a sampling distribution.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n\n\nProbability and probability distribution\nWhat is the probability of buying a bag with exactly five yellow candies? In statistical terminology, what is the probability of drawing a sample with five yellow candies as sample statistic outcome? This probability is the proportion of all possible samples that we could have drawn that happen to contain five yellow candies.\nOf course, the probability of a sample bag with exactly five yellow candies depends on the share of yellow candies in the population of all candies. Figure @ref(fig:probability-distribution) displays the probabilities of a sample bag with a particular number of yellow candies if twenty per cent of the candies in the population are yellow. You can adjust the population share of yellow candies to see what happens.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\nThe sampling distribution tells us all possible samples that we could have drawn. We can use the distribution of all samples to get the probability of buying a bag with exactly five yellow candies from the sampling distribution: We divide the number of samples with five yellow candies by the total number of samples we have drawn. For example, if 26 out of all 1000 samples have five yellow candies, the proportion of samples with five yellow candies is 26 / 1000 = 0.026. Then, the probability of drawing a sample with five yellow candies is 0.026 (we usually write: .026).\nIf we change the frequencies in the sampling distribution into proportions, we obtain the probability distribution of the sample statistic: A sampling space with a probability (between 0 and 1) for each outcome of the sample statistic. Because we are usually interested in probabilities, sampling distributions tend to have proportions, that is probabilities, instead of frequencies on the vertical axis. See Figure @ref(fig:expected-value) for an example.\nFigure @ref(fig:probability-distribution) displays the probability distribution of the number of yellow candies per bag of ten candies. This is an example of a discrete probability distribution because only a limited number of outcomes are possible. It is feasible to list the probability of each outcome separately.\nThe sampling distribution as a probability distribution conveys very important information. It tells us which outcomes we can expect, in our example, how many yellow candies we may find in our bag of ten candies. Moreover, it tells us the probability that a particular outcome may occur. If the sample is drawn from a population in which 20% of candies are yellow, we are quite likely to find zero, one, two, three, or four yellow candies in our bag. A bag with five yellow candies would be rare, six or seven candies would be very rare, and a bag with more than seven yellow candies is extremely unlikely but not impossible. If we buy such a bag, we know that we have been extremely lucky.\nWe may refer to probabilities both as a proportion, that is, a number between 0 and 1, and as a percentage: a number between 0% and 100%. Proportions are commonly considered to be the correct way to express probabilities. When we talk about probabilities, however, we tend to use percentages; we may, for example, say that the probabilities are fifty-fifty.\n\n\nExpected value or expectation\nWe haven’t yet thought about the value that we are most likely to encounter in the sample that we are going to draw. Intuitively, it must be related to the distribution of colours in the population of candies from which the sample was drawn. In other words, the share of yellow candies in the factory’s stock from which the bag was filled or in the machine that produces the candies, seems to be relevant to what we may expect to find in our sample.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\nIf the share of yellow candies in the population is 0.20 (or 20%), we expect one out of each five candies in a bag (sample) to be yellow. In a bag with 10 candies, we would expect two candies to be yellow: one out of each five candies or the population proportion times the total number of candies in the sample = 0.20 * 10 = 2.0. This is the expected value.\nThe expected value of the proportion of yellow candies in the sample is equal to the proportion of yellow candies in the population. If you carefully inspect a sampling distribution (Figure @ref(fig:expected-value)), you will see that the expected value also equals the mean of the sampling distribution. This makes sense: Excess yellow candies in some bags must be compensated for by a shortage in other bags.\nThus we arrive at the definition of the expected value of a random variable:\n\n\n\nIn our example, the random variable is a sample statistic, more specifically, the number of yellow candies in a sample.\nThe sampling distribution is an example of a probability distribution, so, more generally, the expected value is the average of a probability distribution. The expected value is also called the expectation of a probability distribution.\n\n\nUnbiased estimator\nNote that the expected value of the proportion of yellow candies in the bag (sample statistic) equals the true proportion of yellow candies in the candy factory (population statistic). For this reason, the sample proportion is an unbiased estimator of the proportion in the population. More generally, a sample statistic is called an unbiased estimator of the population statistic if the expected value (mean of the sampling distribution) is equal to the population statistic. By the way, we usually refer to the population statistic as a parameter.\nMost but not all sample statistics are unbiased estimators of the population statistic. Think, for instance, of the actual number of yellow candies in the sample. This is certainly not an unbiased estimator of the number of yellow candies in the population. Because the population is so much larger than the sample, the population must contain many more yellow candies than the sample. If we were to estimate the number in the population (the parameter) from the number in the sample—for instance, we estimate that there are two yellow candies in the population of all candies because we have two in our sample of ten—we are going to vastly underestimate the number in the population. This estimate is downward biased: It is too low.\nIn contrast, the proportion in the sample is an unbiased estimator of the population proportion. That is why we do not use the number of yellow candies to generalize from our sample to the population. Instead, we use the proportion of yellow candies. You probably already did this intuitively.\nSometimes, we have to adjust the way in which we calculate a sample statistic to get an unbiased estimator. For instance, we must calculate the standard deviation and variance in the sample in a special way to obtain an unbiased estimate of the population standard deviation and variance. The exact calculation need not bother us, because our statistical software takes care of that. Our software only uses unbiased estimators.\n\n\nRepresentative sample\nBecause the share of yellow candies in the population represents the probability of drawing a yellow candy, we also expect 20% of the candies in our bag to be yellow. For the same reason we expect the shares of all other colours in our sample bag to be equal to their shares in the population. As a consequence, we expect a random sample to resemble the population from which it is drawn.\nA sample is representative of a population (in the strict sense) if variables in the sample are distributed in the same way as in the population. Of course, we know that a random sample is likely to differ from the population due to chance, so the actual sample that we have drawn is usually not representative of the population in the strict sense.\nBut we should expect it to be representative, so we say that it is in principle representative or representative in the statistical sense of the population. We can use probability theory to account for the misrepresentation in the actual sample that we draw. This is what we do when we use statistical inference to construct confidence intervals and test null hypotheses, as we will learn in later chapters.\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::"
  },
  {
    "objectID": "01-samplingdistr.html#cont-random-var",
    "href": "01-samplingdistr.html#cont-random-var",
    "title": "1  Sampling Distribution: How Different Could My Sample Have Been?",
    "section": "1.3 A Continuous Random Variable: Overweight And Underweight.",
    "text": "1.3 A Continuous Random Variable: Overweight And Underweight.\nLet us now look at another variable: the weight of candies in a bag. The weight of candies is perhaps more interesting to the average consumer than candy colour because candy weight is related to calories.\n\nContinuous variable\nWeight is a continuous variable because we can always think of a new weight between two other weights. For instance, consider two candy weights: 2.8 and 2.81 grams. It is easy to see that there can be a weight in between these two values, for instance, 2.803 grams. Between 2.8 and 2.803 we can discern an intermediate value such as 2.802. In principle, we could continue doing this endlessly, e.g., find a weight between 2.80195661 and 2.80195662 grams even if our scales may not be sufficiently precise to measure any further differences. It is the principle that counts. If we can always think of a new value in between two values, the variable is continuous.\n\n\n\n\n\nContinuous sample statistic\nWe are not interested in the weight of a single candy. If a relatively light candy is compensated for by a relatively heavy candy in the same bag, we still get the calories that we want. We are interested in the average weight of all candies in our sample bag, so average candy weight in our sample bag is our key sample statistic. We want to say something about the probabilities of average candy weight in the samples of candies that we can draw. Can we do that?\n\n\n\nWhen we turn to the probabilities of getting samples with a particular average candy weight, we run into problems with a continuous sample statistic. If we would want to know the probability of drawing a sample bag with an average candy weight of 2.8 grams, we should exclude sample bags with an average candy weight of 2.81 grams, or 2.801 grams, or 2.8000000001 grams, and so on. In fact, we are very unlikely to draw a sample bag with an average candy weight of exactly 2.8 grams, that is, with an infinite number of zeros trailing 2.8. In other words, the probability of such a sample bag is for all practical purposes zero and negligible.\nThis applies to every average candy weight, so all probabilities are virtually zero. The probability distribution of the sampling space, that is, of all possible outcomes, is going to be very boring: just (nearly) zeros. And it will take forever to list all possible outcomes within the sampling space, because we have an infinite number of possible outcomes. After all, we can always find a new average candy weight between two selected weights.\n\n\nProbability density\nWith a continuous sample statistic, we must look at a range of values instead of a single value. We can meaningfully talk about the probability of having a sample bag with an average candy weight of at least 2.8 grams or at most 2.8 grams. We choose a threshold, in this example 2.8 grams, and determine the probability of values above or below this threshold. We can also use two thresholds, for example the probability of an average candy weight between 2.75 and 2.85 grams. This is probably what you were thinking of when I referred to a bag with 2.8 grams as average candy weight.\nIf we cannot determine the probability of a single value, which we used to depict on the vertical axis in a plot of a sampling distribution, and we have to link probabilities to a range of values on the x axis, for example, average candy weight above/below 2.8 grams, how can we display probabilities? We have to display a probability as an area between the horizontal axis and a curve. This curve is called a probability density function, so if there is a label to the vertical axis of a continuous probability distribution, it is “Probability density” instead of “Probability”.\nFigure @ref(fig:p-values) shows an example of a continuous probability distribution for the average weight of candies in a sample bag. This is the familiar normal distribution so we could say that the normal curve is the probability density function here. The total area under this curve is set to one, so the area belonging to a range of sample outcomes (average candy weight) is 1 or less, as probabilities should be.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\nA probability density function can give us the probability of values between two thresholds. It can also give us the probability of values up to (and including) a threshold value, which is known as a left-hand probability, or the probability of values above (and including) a threshold value, which is called a right-hand probability. In a null hypothesis significance test (Chapter @ref(hypothesis)), right-hand and left-hand probabilities are used to calculate p values.\nWhy did I put (and including) between parentheses? It does not really matter whether we add the exact boundary value (2.8 grams) to the probability on the left or on the right because the probability of getting a bag with average candy weight at exactly 2.8 grams (with a very long trail of zero decimals) is negligible.\nAre you struggling with the idea of areas instead of heights (values on the vertical axis) as probabilities? Just realize that we could use the area of a bar in a histogram instead of the height as indication of the probability in discrete probability distributions, for example, Figure @ref(fig:expected-value). The bars in a histogram are all equally wide, so (relative) differences between bar areas are equal to differences in bar height.\n\n\nProbabilities always sum to 1\nWhile you were playing with Figure @ref(fig:p-values), you may have noticed that displayed probabilities always add up to one. This is true for every probability distribution because it is part of the definition of a probability distribution.\nIn addition, you may have realized that we can use probability distributions in two ways. We can use them to say how likely or unlikely we are to draw a sample with the sample statistic value in a particular range. For example, what is the chance that we draw a sample bag with average candy weight over 2.9 grams? But we can also use a probability distribution to find the threshold values that separate the top ten per cent or the bottom five per cent in a distribution. If we want a sample bag with highest average candy weight, say, belonging to the ten per cent bags with highest average candy weight, what should be the minimum average candy weight in the sample bag?\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::"
  },
  {
    "objectID": "01-samplingdistr.html#concluding-remarks",
    "href": "01-samplingdistr.html#concluding-remarks",
    "title": "1  Sampling Distribution: How Different Could My Sample Have Been?",
    "section": "1.4 Concluding Remarks",
    "text": "1.4 Concluding Remarks\nA communication scientist wants to know whether children are sufficiently aware of the dangers of media use. On a media literacy scale from one to ten, an average score of 5.5 or higher is assumed to be sufficient.\nIf we translate this to the simple candy bag example, we realize that the outcome in our sample does not have to be the true population value, for example twenty per cent. If twenty per cent of all candies in the population are yellow, we could very well draw a sample bag with fewer or more than twenty per cent yellow candies.\nAverage media literacy, then, can exceed 5.5 in our sample of children, even if average media literacy is below 5.5 in the population or the other way around. How we decide on this is discussed in later chapters.\n\nSample characteristics as observations\nPerhaps the most confusing aspect of sampling distributions is the fact that samples are our cases (units of analysis) and sample characteristics are our observations. We are accustomed to think of observations as measurements on empirical things such as people or candies. We perceive each person or each candy as a case and we observe a characteristic that may change across cases (a variable), for instance the colour or weight of a candy.\nIn a sampling distribution, however, we observe samples (cases) and measure a sample statistic as the (random) variable. Each sample adds one observation to the sampling distribution and its sample statistic value is the value added to the sampling distribution.\n\n\nMeans at three levels\nIf we are dealing with the proportion of yellow candies in a sample (bag), the sample statistic is a proportion and we want to know the proportion of yellow candies in the population. The sampling distribution collects a large number of sample proportions. The mean of the proportions in the sampling distribution (expected value) equals the proportion of yellow candies in the population, because a sample proportion is an unbiased estimator of the population proportion.\nThings become a little confusing if we are interested in a sample mean, such as the average weight of candies in a sample bag. Now we have means at three levels: the population, the sampling distribution, and the sample.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\nThe sampling distribution, here, is a distribution of sample means but the sampling distribution itself also has a mean, which is called the expected value or expectation of the sampling distribution. Don’t let this confuse you. The mean of the sampling distribution is the average of the average weight of candies in every possible sample bag. This mean of means has the same value as our first mean, namely the average weight of the candies in the population because a sample mean is an unbiased estimator of the population mean.\nRemember this: The population and the sample consist of the same type of observations. In the current example, we are dealing with a sample and a population of candies. In contrast, the sampling distribution is based on a different type of observation, namely samples, for example, sample bags of candies.\nThe sampling distribution is the crucial link between the sample and the population. On the one hand the sampling distribution is connected to the population because the population statistic (parameter), for example, average weight of all candies, is equal to the mean of the sampling distribution. On the other hand, it is linked to the sample because it tells us which sample means we will find with what probabilities. We need the sampling distribution to make statements about the population based on our sample.\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::"
  },
  {
    "objectID": "01-samplingdistr.html#test-your-understanding",
    "href": "01-samplingdistr.html#test-your-understanding",
    "title": "1  Sampling Distribution: How Different Could My Sample Have Been?",
    "section": "1.5 Test Your Understanding",
    "text": "1.5 Test Your Understanding\nFigure @ref(fig:sampling-distribution-summary1) simulates drawing random samples from a candy factory’s stock of candies. We are interested in the colour of the candies in our sample. The top-left histogram shows the distribution of candies according to colour in the population. Draw some samples, have a look at the number of yellow candies in each sample, and inspect the sampling distribution.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n ::: {.cell type=‘rmdquestion’}\n:::\n\nAnswers\n\n\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 1.\n\nIf we would really sample candies, the population could be a candy factory’s stock of candies or the factory’s machine producing the candies from which we sample. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 2.\n\nThe numbers on the horizontal axis of the bottom-right histogram represent the number of yellow candies in the sample(s) that we have drawn. The variable for which these numbers are values is called a sample statistic. Together, these numbers constitute the sampling space, that is, all values that the sample statistic “Number of yellow candies in the sample” can take.\nA sample statistic is a characteristic of a sample; we have a sample statistic score for each sample that we draw. The sample, then, is the unit of analysis or case for this variable and this histogram. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 3.\n\nThe sample characteristic is the number of yellow candies in our sample. Because our sample contains ten candies, this number can vary between zero and ten. This range of values is called the sampling space. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 4.\n\nIf the proportion of yellow candies is .2 in the population, we expect that this is the proportion of yellow candies that is most likely in our sample. Therefore, samples with this proportion of yellow candies are most frequent if we draw many samples. Each sample contains ten candies, so a proportion of .2 equals two yellow candies in a sample. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 5.\n\nNo. \nLet us assume that all colours have equal shares in the candy population, like the example in the figure. A sample is representative of the population (in the strict sense) with respect to candy colour if the colors have equal shares also in the sample. This is the only sample that is representative of the candy population with respect to candy colours. But most of the samples that we draw have an unequal distribution of colours. These samples are not representative of the population. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 6.\n\nAs we see when we draw several samples, the number of yellow candies varies across samples. This variation arises because we draw samples at random. So the scores of samples on the sample statistic is random. That is a good reason for calling it a random variable. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 7.\n\nThe sampling distribution in this figure shows average candy weights for all possible or a very large number of random samples drawn from a population of candies, for example, a factory’s stock. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 8.\n\nThe population is not depicted here, so we must infer average candy weight in the population from the sampling distribution. If the sample statistic is an unbiased estimator of the population statistic (this is the case for a sample mean), the average of the sampling distribution (this is called the expected value) is equal to the population statistic.\nIn the current example, the average of the sampling distribution of average sample bag candy weights is equal to average candy weight in the population.\nThe sampling distribution depicted here is symmetrical, so the average equals the median value, so half of the observed average sample candy weights are below this value and the other half is above this value.\nIf one of the slider handles demarcates half of the probability from the other half, this handle indicates the average of the sampling distribution. In this example, the average is 2.8 (grams). \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 9.\n\nIf you set the left-hand slider handle to 2.0 and the right-hand handle to 2.9, the blue area represents the probability of drawing a sample with average candy weight between 2.0 and 2.9 grams. The value of the probability is depicted in the blue box within the graph. It is .475. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 10.\n\nThis probability is (virtually) zero. If we would measure weight with very high precision, no candy bag would have average candy weigth of exactly 2.9 grams, that is, 2.90000000000000000000000000(and so on) grams. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 11.\n\nSee the answer to Question 10. A variable is continuous if we can, in principle, always find a new value between two values. This applies to average weight as a variable, because we can in principle always use more decimal places in our measurement to find a weight that is between two other weights. For example, between 2.90000 and 2.90001 we can think of the weight 2.900005.\nIf the sample statistic is a continuous variable, such as average candy weight in the sample, the probability distribution for this sample statistic is continuous. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 12.\n\nIt makes no sense to speak of the probability (vertical axis) that average candy weight in a sample (horizontal axis) has one particular value. Because average weight is a continuous variable (see Exercise 11), the probability of one particular outcome value is (virtually) zero (see Exercise 10). If we would draw the probabilities on the vertical axis, we would have a flat line (at zero) instead of a curve.\nInstead of probabilities of single outcome values, we are interested in probabilities of ranges or intervals of outcome values if we have a continuous random variable. For example, the probability of a sample with average candy weight between 2.8 and 2.9 grams. The probability of a range of outcome values is depicted as a surface below a probability density function. This is what our graph of a continuous sampling distribution shows, so the vertical axis is labelled ‘probability density’ instead of ‘probability’. \n\n:::"
  },
  {
    "objectID": "01-samplingdistr.html#take-home-points",
    "href": "01-samplingdistr.html#take-home-points",
    "title": "1  Sampling Distribution: How Different Could My Sample Have Been?",
    "section": "1.6 Take-Home Points",
    "text": "1.6 Take-Home Points\n\nValues of a sample statistic vary across random samples from the same population. But some values are more probable than other values.\nThe sampling distribution of a sample statistic tells us the probability of drawing a sample with a particular value of the sample statistic or a particular minimum and/or maximum value.\nIf a sample statistic is an unbiased estimator of a parameter, the parameter value equals the average of the sampling distribution, which is called the expected value or expectation.\nFor discrete sample statistics, the sampling distribution tells us the probability of individual sample outcomes. For continuous sample statistics, it tells us the probability density, which gives us the probability of drawing a sample with an outcome that is at least or at most a particular value, or an outcome that is between two values."
  },
  {
    "objectID": "02-probability.html#boot-approx",
    "href": "02-probability.html#boot-approx",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.1 The Bootstrap Approximation of the Sampling Distribution",
    "text": "2.1 The Bootstrap Approximation of the Sampling Distribution\nThe first way to obtain a sampling distribution is still based on the idea of drawing a large number of samples. However, we only draw one sample from the population for which we collect data. As a next step, we draw a large number of samples from our initial sample. The samples drawn in the second step are called bootstrap samples. The technique was developed by Bradley Efron (RefWorks:3956?; RefWorks:3957?). For each bootstrap sample, we calculate the sample statistic of interest and we collect these as our sampling distribution. We usually want about 5,000 bootstrap samples for our sampling distribution.\n\n\n\n\nIn Figure @ref(fig:bootstrapping), an initial sample (left panel) has been drawn from a population containing five candy colours in equal proportions.\n ::: {.cell type=‘rmdquestion’}\n\n\nHow large is a bootstrap sample in Figure @ref(fig:bootstrapping)? Use the Bootstrap one sample button. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhat element in Figure @ref(fig:bootstrapping) represents the true sampling distribution in this example? If in doubt, see Figure @ref(fig:probability-distribution). \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nDoes the bootstrap sampling distribution resemble the true sampling distribution? Use the “Bootstrap 5,000 samples” button and justify your answer. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nDraw a new initial sample. This sample is probably not representative of the distribution of candy colour in the population. What happens to the bootstrap samples and the bootstrap sampling distribution? \n\n:::\n\n\n\n\nThe bootstrap concept refers to the story in which Baron von Münchhausen saves himself by pulling himself and his horse by his bootstraps (or hair) out of a swamp. In a similar miraculous way, bootstrap samples resemble the sampling distribution even though they are drawn from a sample instead of the population. This miracle requires some explanation and it does not work always, as we will discuss in the remainder of this section.\nPicture: Baron von Münchhausen pulls himself and his horse out of a swamp. Theodor Hosemann (1807-1875), public domain, via Wikimedia Commons\n\n\n\nSampling with and without replacement\nAs we will see in Chapter @ref(param-estim), for example Section @ref(precisionsesamplesize), the size of a sample is very important to the shape of the sampling distribution. The sampling distribution of samples with twenty-five cases can be very different from the sampling distribution of samples with fifty cases. To construct a sampling distribution from bootstrap samples, the bootstrap samples must be exactly as large as the original sample.\nHow can we draw many different bootstrap samples from the original sample if each bootstrap sample must contain the same number of cases as the original sample?\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhat are the differences between sampling with and without replacement (Figure @ref(fig:replacement))? Press the buttons Draw Without Replacement and Draw With Replacement once or several times to see the differences. \n\n:::\n\nIf we allow every case in the original sample to be sampled only once, each bootstrap sample contains all cases of the original sample, so it is an exact copy of the original sample. Thus, we cannot create different bootstrap samples.\nBy the way, we often use the type of sampling described above, which is called sampling without replacement. If a person is (randomly) chosen for our sample, we do not put this person back into the population so she or he can be chosen again. We want our respondents to fill out our questionnaire only once or participate in our experiment only once.\nIf we do allow the same person to be chosen more than once, we sample with replacement. The same person can occur more than once in a sample. Bootstrap samples are sampled with replacement from the original sample, so one bootstrap sample may differ from another. Some cases in the original sample may not be sampled for a bootstrap sample while other cases are sampled several times. You probably have noticed this in Figure @ref(fig:replacement). Sampling with replacement allows us to obtain different bootstrap samples from the original sample, and still have bootstrap samples of the same size as the original sample.\nIn conclusion, we sample bootstrap samples in a different way (with replacement) than participants for our research (without replacement).\n\n\nLimitations to bootstrapping\nDoes the bootstrapped sampling distribution always reflect the true sampling distribution?\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhen does the bootstrap sampling distribution (yellow histogram) reflect the true sampling distribution (grey histogram) better: at small or large sample sizes? Play with sample size in Figure @ref(fig:bootstrap-lim) to check your answer. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nHow does sample size relate to representativeness of the sample in terms of the proportion of yellow candies? Note that twenty per cent of the candies in the population are yellow. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nIf you use a very small sample size, it may happen that there is no yellow histogram in the bottom graph. Why is that? \n\n:::\n\nWe can create a sampling distribution by sampling from our original sample with replacement. It is hardly a miracle that we obtain different samples with different sample statistics if we sample with replacement. Much more miraculous, however, is that this bootstrap distribution resembles the true sampling distribution that we would get if we draw lots of samples directly from the population.\nDoes this miracle always happen? No. The original sample that we have drawn from the population must be more or less representative of the population. The variables of interest in the sample should be distributed more or less the same as in the population. If this is not the case, the sampling distribution may give a distorted view of the true sampling distribution. This is the main limitation to the bootstrap approach to sampling distributions.\nA sample is more likely to be representative of the population if the sample is drawn in a truly random fashion and if the sample is large. But we can never be sure. There always is a chance that we have drawn a sample that does not reflect the population well.\n\n\nAny sample statistic can be bootstrapped\nThe big advantage of the bootstrap approach (bootstrapping) is that we can get a sampling distribution for any sample statistic that we are interested in. Every statistic that we can calculate for our original sample can also be calculated for each bootstrap sample. The sampling distribution is just the collection of the sample statistics calculated for all bootstrap samples.\nBootstrapping is more or less the only way to get a sampling distribution for the sample median, for example, the median weight of candies in a sample bag. We may create sampling distributions for the wildest and weirdest sample statistics, for instance the difference between sample mean and sample median squared. I would not know why you would be interested in the squared difference of sample mean and median, but there are very interesting statistics that we can only get at through bootstrapping. A case in point is the strength of an indirect effect in a mediation model (Chapter @ref(mediation)).\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 1.\n\nA bootstrap sample must be just as large as the initial sample. In this example, the initial sample contains twenty-five candies, so the bootstrap sample must also contain twenty-five candies.\nAs we will find out later, the size of a sample is very important to the sampling distribution, so we must draw bootstrap samples with exactly the same number of observations as the initial sample. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 2.\n\nThe sampling distribution of a sample proportion is an exact distribution (named binomial distribution): the probabilities of every number or proportion of yellow candies in the sample can be calculated. The results are displayed as a grey histogram at the right of the figure. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 3.\n\nThe proportion of yellow candies in the (first) initial sample is .2, that is, five out of twenty-five candies in the sample are yellow.\nThe initial sample is representative of the population with respect to candy colour, because the proportion of yellow candies in the population is also .2.\nAs a result, the bootstrapped sampling distribution (yellow histogram) is very similar to the true (exact) sampling distribution (grey histogram). \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 4.\n\nIf the proportion of yellow candies in the original sample is close to .2, that is, five out of twenty-five candies in the sample are yellow, the bootstrapped sampling distribution (yellow histogram) is very similar to the true (exact) sampling distribution (grey histogram).\nIf there are considerably less or more than five yellow candies in the sample, however, the bootstrapped sampling distribution is quite different from the true sampling distribution. Conclusions based on the bootstrapped sampling distribution will be wrong. Especially the mean (horizontal location) of the bootstrapped sampling distribution is different. The shape of the distribution may still be nearly the same.\nNote that the true sampling distribution, represented by the grey histogram, remains the same because the proportion of yellow candies in the population remains the same, namely 20 per cent. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 5.\n\nIf we draw a sample without replacement from our initial sample of the same size as the initial sample, the new sample must contain all observations from the initial sample. As a result, the new sample is identical to the initial sample. All samples that we draw are identical. This does not provide an interesting sampling distribution.\nDrawing with replacement, an observation can be drawn more than once. As a result, the same candy number may appear more than once in the new sample. Otherwise, we could never have more candies of a particular color in the bootstrapped sample than in the original sample (five candies of each color). Each new sample drawn with replacement from the original sample can be different, so the proportion of yellow candies varies across these bootstrap samples. We can create a meaningful sampling distribution from these varying proportions of yellow candies. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 6.\n\nIf you change sample size repeatedly between 15 and 45, you will see that the bootstrapped sampling distribution (yellow histogram) jumps around the true sampling distribution (grey histogram).\nFor relatively small sample sizes, the bootstrapped sampling distribution is often quite different from the true sampling distribution.\nAt larger sample sizes, say between 120 and 150, the bootstrapped sampling distribution overlaps the true sampling distribution much more frequently. So for larger samples, we can trust the bootstrapped sampling distribution more. But even then, it can sometimes be quite off the mark. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 7.\n\nThe proportion of yellow candies in larger samples is more often close to the proportion in the population: 0.2. This is the reason that the bootstrapped sampling distribution resembles the true sampling distribution more often for a larger sample. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 8.\n\nIf we draw an initial sample without any yellow candies, none of the bootstrap samples can include yellow candies. As a result, the count of samples with yellow candies is always zero.\nThe smaller the initial sample, the greater the chance of having a sample without yellow candies. \n\n:::"
  },
  {
    "objectID": "02-probability.html#boot-spss",
    "href": "02-probability.html#boot-spss",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.2 Bootstrapping in SPSS",
    "text": "2.2 Bootstrapping in SPSS\n\nInstructions\n\n\n\n\n\n\n\n\nIn principle, any sample statistic can be bootstrapped. SPSS, however, does not bootstrap sample statistics that we had better not use because they give bad (biased) results. For example, SPSS does not bootstrap the minimum value, maximum value or the range between minimum and maximum value of a variable.\nSPSS reports bootstrapping results as confidence intervals. We will discuss confidence intervals in detail in the next chapter.\n\n\nExercises\n ::: {.cell type=‘rmdquestion’}\n\n\nDownload the data set candies.sav and use SPSS to bootstrap the t test on average weight of yellow and red candies (the example above). The test is available in the Analyze>Compare Means menu. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nUse the same data set to bootstrap the median of candy weight. Remember that measures of central tendency can be obtained with the Frequencies>Statistics command in the Analyze>Descriptive Statistics menu.\n\nTip: Speed up bootstrapping in SPSS by deselecting the option Display frequency tables. \n\n:::\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Exercise 1.\nSPSS syntax:\n* Exercise 1: Bootstrap different averages.\n* Check data.\nFREQUENCIES VARIABLES=colour weight\n/ORDER=ANALYSIS.\n* Execute independent-samples t test with bootstrap.\nBOOTSTRAP\n/SAMPLING METHOD=SIMPLE\n/VARIABLES TARGET=weight INPUT=colour\n/CRITERIA CILEVEL=95 CITYPE=BCA NSAMPLES=5000\n/MISSING USERMISSING=EXCLUDE.\nT-TEST GROUPS=colour(4 5)\n/MISSING=ANALYSIS\n/VARIABLES=weight\n/CRITERIA=CI(.95).\nCheck data:\nThere are no impossible values on the two variables.\nInterpret the results:\nThe table “Bootstrap for Independent Samples Test” contains the results that we are interested in.\nLevene’s test on homogeneity of variances is not statistically significant, so we may assume that the population variances of red and yellow candy weight are equal. So we interpret the top row in table “Bootstrap for Independent Samples Test”.\nThe mean difference between red and yellow candy weight is 0.05 grams. In our sample, red candies are just a little heavier than yellow candies.\nThe bootstrapped 95% confidence interval for this difference is -0.11 to 0.21. With 95% confidence, we can say that red candies can be on average 0.11 grams lighter than yellow candies or up to 0.21 grams heavier. We cannot tell which of the two are heavier in the population with sufficient confidence.\nNote that your results can be slightly different because bootstrapping creates random samples. \n\n:::\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Exercise 2.\nSPSS syntax:\n* Exercise 2: Bootstrap on median candy weight.\n* Check data.\nFREQUENCIES VARIABLES=weight\n/ORDER=ANALYSIS.\n* Bootstrap the median.\nBOOTSTRAP\n/SAMPLING METHOD=SIMPLE\n/VARIABLES INPUT=weight\n/CRITERIA CILEVEL=95 CITYPE=BCA NSAMPLES=5000\n/MISSING USERMISSING=EXCLUDE.\nFREQUENCIES VARIABLES=weight\n/FORMAT=NOTABLE\n/STATISTICS=MEDIAN\n/ORDER=ANALYSIS.\nCheck data:\nThere are no impossible values on the weight variable.\nInterpret the results:\nMedian candy weight in the sample is 2.81 grams. With 95% confidence, we expect median candy weight to be between 2.78 and 2.92 grams in the population of all candies.\nThe 95% interval borders can be slightly different because bootstrapping takes random samples. \n\n:::"
  },
  {
    "objectID": "02-probability.html#exact-approaches-to-the-sampling-distribution",
    "href": "02-probability.html#exact-approaches-to-the-sampling-distribution",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.3 Exact Approaches to the Sampling Distribution",
    "text": "2.3 Exact Approaches to the Sampling Distribution\nA second approach to constructing a sampling distribution has implicitly been demonstrated in the preceding section on bootstrapping (Section @ref(boot-approx)) and the section on probability distributions (Section @ref(probdistribution)). In these sections, we calculated the true sampling distribution of the proportion of yellow candies in a sample from the probabilities of the colours. If we know or think we know the proportion of yellow candies in the population, we can exactly calculate the probability that a sample of ten candies includes one, two, three, or ten yellow candies. See the section on discrete random variables for details (Section @ref(discreterandomvariable)).\n\n\n\n\n\nNumber of heads for a toss of three coins.\n\n\n\n\nOutcome\n\n\nCombination\n\n\nProbability: Combination\n\n\nProbability: Outcome\n\n\n\n\n\n\n0\n\n\ntail-tail-tail\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n1/8 = .125\n\n\n\n\n1\n\n\ntail-tail-head\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n\n\n\n1\n\n\nhead-tail-tail\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n\n\n\n1\n\n\ntail-head-tail\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n3/8 = .375\n\n\n\n\n2\n\n\nhead-head-tail\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n\n\n\n2\n\n\nhead-tail-head\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n\n\n\n2\n\n\ntail-head-head\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n3/8 = .375\n\n\n\n\n3\n\n\nhead-head-head\n\n\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n1/8 = .125\n\n\n\n\nTotal\n\n\n8\n\n\n\n\n1.000\n\n\n\n\n\nHow does an exact aproach to the sampling distribution work?\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nExplain the meaning of the entries in the column Combination and how they relate to the entries in the Outcome column. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nExplain how the combinations relate to the probabilities. \n\n:::\n\nThe calculated probabilities of all possible sample statistic outcomes give us an exact approach to the sampling distribution. Note that I use the word approach instead of approximation here because the obtained sampling distribution is no longer an approximation, that is, more or less similar to the true sampling distribution. No, it is the true sampling distribution itself.\n\nExact approaches for categorical data\nAn exact approach lists and counts all possible combinations. This can only be done if we work with discrete or categorical variables. For an unlimited number of categories, we cannot list all possible combinations.\nA proportion is based on frequencies and frequencies are discrete (integer values), so we can use an exact approach to create a sampling distribution for one proportion such as the proportion of yellow candies in the example above. The exact approach uses the binomial probability formula to calculate probabilities. Consult the internet if you want to know this formula; we are not going to use it here.\nExact approaches are also available for the association between two categorical (nominal or ordinal) variables in a contingency table: Do some combinations of values for the two variables occur relatively frequently? For example, are yellow candies more often sticky than red candies? If candies are either sticky or not sticky and they have one out of a limited set of colours, we have two categorical variables. We can create an exact probability distribution for the combination of colour and stickiness. The Fisher-exact test is an example of an exact approach to the sampling distribution of the association between two categorical variables.\n\n\nComputer-intensive\nThe exact approach can be applied to discrete variables because they have a limited number of values. Discrete variables are usually measured at the nominal or ordinal level. If the number of categories becomes large, a lot of computing time can be needed to calculate the probabilities of all possible sample statistic outcomes. Exact approaches are said to be computer-intensive.\nIt is usually wise to set a limit to the time you allow your computer to work on an exact sampling distribution because otherwise the problem may keep your computer occupied for hours or days.\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 1.\n\nThe column “Combination” lists all posible outcomes if we toss three coins.\nThe sample statistic is the number of heads in a throw of three coins, which is reported in the “Outcome” column. It simply counts the number of heads that appear in the combination.\nThis number can range from zero to three. This is the sampling space. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 2.\n\nThere are eight combinations. If the coins are fair, each combination has the same probability of appearing, namely 1/8 = .125. We sum the probabilities for all combinations that have the same outcome, namely the same number of heads.\nThus we arrive at the probability of having no heads in a throw (p = .125), one head (p = .375), and so on. \n\n:::"
  },
  {
    "objectID": "02-probability.html#SPSS-exact",
    "href": "02-probability.html#SPSS-exact",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.4 Exact Approaches in SPSS",
    "text": "2.4 Exact Approaches in SPSS\n\nInstructions\n\n\n\n\n\n\n\n\n\n\nExercises\n ::: {.cell type=‘rmdquestion’}\n\n\nDownload the data set candies.sav and use SPSS to apply a Fisher-exact test to the association between candy colour and candy stickiness. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWith the same data, apply a Fisher-exact test to the association between candy colour and candy spottiness. \n\n:::\n\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Exercise 1.\nSPSS syntax:\n* Exact test on the relation between candy colour\n* and candy stickiness.\nCROSSTABS\n/TABLES=colour BY sticky\n/FORMAT=AVALUE TABLES\n/STATISTICS=CHISQ PHI\n/CELLS=COUNT COLUMN\n/COUNT ROUND CELL\n/METHOD=EXACT TIMER(5).\nCheck data:\nThe contingency table does not show any impossible values for the two categorical variables.\nInterpret the results:\nThere is a strong association (Cramer’s V = .52) between candy colour and candy stickiness, which is statistically significant, p = .010 (exact). If we look at the percentages in the contingency table, we see that yellow and red candies are less often sticky than blue, green, and orange candies. \n\n:::\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Exercise 2.\nSPSS syntax:\n* Exact test on the relation between candy colour\n* and candy spottiness.\nCROSSTABS\n/TABLES=colour BY spotted\n/FORMAT=AVALUE TABLES\n/STATISTICS=CHISQ PHI\n/CELLS=COUNT COLUMN\n/COUNT ROUND CELL\n/METHOD=EXACT TIMER(5).\nCheck data:\nThe contingency table does not show any impossible values for the two categorical variables.\nInterpret the results:\nThere is a weak association (Cramer’s V = .27) between candy colour and candy spottiness, which is not statistically significant, p = .480 (exact) or (using chi-square) p = .555.\nCandy colour may be relevant to having spots (weak association) but we are unsure (not statistically significant). \n\n:::"
  },
  {
    "objectID": "02-probability.html#theoretical-approximations-of-the-sampling-distribution",
    "href": "02-probability.html#theoretical-approximations-of-the-sampling-distribution",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.5 Theoretical Approximations of the Sampling Distribution",
    "text": "2.5 Theoretical Approximations of the Sampling Distribution\nBecause bootstrapping and exact approaches to the sampling distribution require quite a lot of computing power, these methods were not practical in the not so very distant pre-computer age. In those days, mathematicians and statisticians discovered that many sampling distributions look a lot like known mathematical functions. For example, the sampling distribution of the sample mean can be quite similar to the well-known bell-shape of the normal distribution or the closely related (Student) t distribution. The mathematical functions are called theoretical probability distributions. Most statistical tests use a theoretical probability distribution as approximation of the sampling distribution.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nFigure @ref(fig:normal-approximation) displays a simulated sampling distribution of sample means and the normal approximation of this distribution (curve). Check if the normal curve is a good approximation of the sampling distribution. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhile checking the distribution, pay special attention to the tails because these are used for significance tests (see Chapter @ref(hypothesis)). The red and green bars represent the 2.5 per cent samples with the lowest or highest average weights. The vertical lines mark the outer 2.5 per cent according to the normal distribution. Do the tail borders of the sampling distribution and normal distribution match? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nGenerate some new sampling distributions to see if the normal function always yields a good approximation. Does the bell-shaped curve fit the histogram? \n\n:::\n\nThe normal distribution is a mathematical function linking continuous scores, e.g., a sample statistic such as the average weight in the sample, to right-hand and left-hand probabilities, that is, to the probability of finding at least, or at most, this score. Such a function is called a probability density function (Section @ref(cont-random-var)).\nWe like to use a theoretical probability distribution as an approximation of the sampling distribution because it is convenient. A computer can calculate probabilities from the mathematical function very quickly. We also like theoretical probability distributions because they usually offer plausible arguments about chance and probabilities.\n\nReasons for a bell-shaped probability distribution\nThe bell shape of the normal distribution makes sense. Our sample of candies is just as likely to be too heavy, as it is too light, so the sampling distribution of the sample mean should be symmetrical. A normal distribution is symmetrical.\nIn addition, it is more likely that our sample bag has an average weight that is near the true average candy weight in the population than an average weight that is much larger or much smaller than the true average. Bags with on average extremely heavy or extremely light candies may occur, but they are extremely rare (we are very lucky or very unlucky). From these intuitions we would expect a bell shape for the sampling distribution.\nFrom this argumentation, we conclude that the normal distribution is a reasonable model for the probability distribution of sample means. Actually, it has been proven that the normal distribution exactly represents the sampling distribution in particular cases, for instance the sampling distribution of the mean of a very large sample.\n\n\nConditions for the use of theoretical probability distributions\nTheoretical probability distributions, then, are plausible models for sampling distributions. They are known or likely to have the same shape as the true sampling distributions under particular circumstances or conditions.\nIf we use a theoretical probability distribution, we must assume that the conditions for its use are met. We have to check the conditions and decide whether they are close enough to the ideal conditions. Close enough is of course a matter of judgement. In practice, rules of thumb have been developed to decide if the theoretical probability distribution can be used.\nFigure @ref(fig:normal-approx-proportion) shows an example in which the normal distribution is a good approximation for the sampling distribution of a proportion in some situations, but not in all situations.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nHow does sample size affect the shape of the sampling distribution? See what happens if you change sample size in the interactive content. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nHow does the population proportion affect the shape of the sampling distribution? See what happens if you change the population proportion in the interactive content. \n\n:::\n\nDo theoretical probability distributions fit the true sampling distribution? As you may have noticed while playing with Figure @ref(fig:normal-approx-proportion), this is not always the case. In general, theoretical probability distributions fit sampling distributions better if the sample is larger. In addition, the population value may be relevant to the fit of the theoretical probability distribution. The sampling distribution of a sample proportion is more symmetrical, like the normal distribution, if the proportion in the population is closer to .5.\nThis illustrates that we often have several conditions for a theoretical probability distribution to fit the sampling distribution. We should evaluate all of them at the same time. In the example of proportions, a large sample is less important if the true proportion is closer to .5 but it is more important for true proportions that are more distant from .5.\nThe rule of thumb for using the normal distribution as the sampling distribution of a sample proportion combines the two aspects by multiplying them and requiring the resulting product to be larger than five. If the probability of drawing a yellow candy is .2 and our sample size is 30, the product is .2 * 30 = 6, which is larger than five. So we may use the normal distribution as approximation of the sampling distribution.\nNote that this rule of thumb uses one minus the probability, if the probability is larger than .5. In other words, it uses the smaller of two probabilities: the probability that an observation has the characteristic and the probability that it has not. For example, if we want to test the probability of drawing a candy that is not yellow, the probability is .8 and we use 1 - 0.8 = 0.2, which is then multiplied by the sample size.\nApart from the normal distribution, there are several other theoretical probability distributions. We have the binomial distribution for a proportion, the t distribution for one or two sample means, regression coefficients, and correlation coefficients, the F distribution for comparison of variances and comparing means for three or more groups (analysis of variance, ANOVA), and the chi-squared distribution for frequency tables and contingency tables.\nFor most of these theoretical probability distributions, sample size is important. The larger the sample, the better. There are additional conditions that must be satisfied such as the distribution of the variable in the population. The rules of thumb are summarized in Table @ref(tab:thumb). Bootstrapping and exact tests can be used if conditions for theoretical probability distributions have not been met. Special conditions apply to regression analysis (see Chapter @ref(moderationcat), Section @ref(regr-inference)).\n\n\n\n\nRules of thumb for using theoretical probability distributions.\n \n  \n    Distribution \n    Sample statistic \n    Minimum sample size \n    Other requirements \n  \n \n\n  \n    Binomial distribution \n    proportion \n    - \n    - \n  \n  \n    (Standard) normal distribution \n    proportion \n    times test proportion (<= .5) >= 5 \n    - \n  \n  \n    (Standard) normal distribution \n    one or two means \n    > 100 \n    OR variable is normally distributed in the population and population standard deviation is known (for each group) \n  \n  \n    t distribution \n    one or two means \n    each group > 30 \n    OR variable is normally distributed in each group's population \n  \n  \n    t distribution \n    (Pearson) correlation coefficient \n    - \n    variables are normally distributed in the population \n  \n  \n    t distribution \n    (Spearman) rank correlation coefficient \n    > 30 \n    - \n  \n  \n    t distribution \n    regression coefficient \n    20+ per independent variable \n    See Chapter 8. \n  \n  \n    F distribution \n    3+ means \n    all groups are more or less of equal size \n    OR all groups have the same population variance \n  \n  \n    F distribution \n    two variances \n    - \n    no conditions for Levene's F test \n  \n  \n    chi-squared distribution \n    row or cell frequencies \n    expected frequency >= 1 and 80% >= 5 \n    contingency table: 3+ rows or 3+ columns \n  \n\n\n\n\n\n\n\nChecking conditions\nRules of thumb about sample size are easy to check once we have collected our sample. By contrast, rules of thumb that concern the scores in the population cannot be easily checked, because we do not have information on the population. If we already know what we want to know about the population, why would we draw a sample and do the research in the first place?\nWe can only use the data in our sample to make an educated guess about the distribution of a variable in the population. For example, if the scores in our sample are clearly normally distributed, it is plausible that the scores in the population are normally distributed.\nIn this situation, we do not know that the population distribution is normal but we assume it is. If the sample distribution is clearly not normally distributed, we had better not assume that the population is normally distributed. In short, we sometimes have to make assumptions when we decide on using a theoretical probability distribution.\nWe could use a histogram of the scores in our sample with a normal distribution curve added to evaluate whether a normal distribution applies. Sometimes, we have statistical tests to draw inferences about the population from a sample that we can use to check the conditions. We discuss these tests in a later chapter.\n\n\nMore complicated sample statistics: differences\nUp to this point, we have focused on rather simple sample statistics such as the proportion of yellow candies or the average weight of candies in a sample. Table @ref(tab:thumb), however, contains more complicated sample statistics.\nIf we compare two groups, for instance, the average weight of yellow and red candies, the sample statistic for which we want to have a sampling distribution must take into account both the average weight of yellow candies and the average weight of red candies. The sample statistic that we are interested in is the difference between the averages of the two samples.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nClick on the button once. Why are these samples called independent? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nClick on the button several times. What exactly is the sample statistic in the histogram at the bottom of the app? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nClick on the button to draw one thousand samples once or more often. Does the sampling distribution look familiar to you? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhat, do you expect, is the mean of the sampling distribution? \n\n:::\n\nIf we draw a sample from both the red and yellow candies in the population, we may calculate the means for both samples and the difference between the two means. For example, the average weight of red candies in the sample bag is 2.76 grams and the average for yellow candies is 2.82 grams. For this pair of samples, the statistic of interest is 2.76 - 2.82 = -0.06, that is, the difference in average weight. If we repeat this many, many times and collect all differences between means in a distribution, we obtain the sampling distribution that we need.\nThe sampling distribution of the difference between two means is similar to a t-distribution, so we may use the latter to approximate the former. Of course, the conditions for using the t distribution must be met.\nIt is important to note that we do not create separate sampling distributions for the average weight of yellow candies and for the average weight of red candies and then look at the difference between the two sampling distributions. Instead, we create one sampling distribution for the statistic of interest, namely the difference between means. We cannot combine different sampling distributions into a new sampling distribution. We will see the importance of this when we discuss mediation (Chapter @ref(mediation)).\n\n\nIndependent samples\nIf we compare two means, there are two fundamentally different situations that are sometimes difficult to distinguish. When comparing the average weight of yellow candies to the average weight of red candies, we are comparing two samples that are statistically independent (see Figure @ref(fig:mean-independent)), which means that we could have drawn the samples separately.\nIn principle, we could distinguish between a population of yellow candies and a population of red candies, and sample yellow candies from the first population and separately sample red candies from the other population. Whether we sampled the colours separately or not does not matter. The fact that we could have done so implies that the sample of red candies is not affected by the sample of yellow candies or the other way around. The samples are statistically independent.\nThis is important for the way in which probabilities are calculated. Just think of the simple example of flipping two coins. The probability of having heads twice in a row is .5 times .5, that is .25, if the coins are fair and the result of the second coin does not depend on the result of the first coin. The second flip is not affected by the first flip.\nImagine that a magnetic field is activated if the first coin lands with heads up and that this magnetic field increases the odds that the second coin will also be heads. Now, the second toss is not independent of the first toss and the probability of getting heads twice is larger than .25.\n\n\nDependent samples\nThe example of a manipulated second toss is applicable to repeated measurements. If we want to know how quickly the yellow colour fades when yellow candies are exposed to sun light, we may draw a sample of yellow candies once and measure the colourfulness of each candy at least twice: at the start and end of some time interval. We compare the colourfulness of a candy at the second measurement to its colourfulness at the first measurement.\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nIn Figure @ref(fig:mean-dependent), use the Sample 1 case button repeatedly to draw a sample of five observations. What is the precise meaning of the numbers on the horizontal axis in the dot plot representing the sample (in the middle of Figure @ref(fig:mean-dependent))? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhy is the sample called dependent or paired? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nDraw 1,000 samples to obtain a sampling distribution. What is the precise meaning of the numbers on the horizontal axis in the histogram of the sampling distribution? \n\n:::\n\nIn this example, we are comparing two means, just like the yellow versus red candy weight example, but now the samples for both measurements are the same. It is impossible to draw the sample for the second measurement independently from the sample for the first measurement if we want to compare repeated measurements. Here, the second sample is fixed once we have drawn the first sample. The samples are statistically dependent; they are paired samples.\nWith dependent samples, probabilities have to be calculated in a different way, so we need a special sampling distribution. In the interactive content above, you may have noticed a relatively simple solution for two repeated measurements. We just calculate the difference between the two measurements for each candy in the sample and use the mean of this new difference variable as the sample statistic that we are interested in. The t-distribution, again, offers a good approximation of the sampling distribution of dependent samples if the samples are not too small.\nFor other applications, the actual sampling distributions can become quite complicated but we do not have to worry about that. If we choose the right technique, our statistical software will take care of this.\n\n\nAnswers\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 1.\n\nThe curve fits the histogram of observed sample means quite well. Discrepancies are mainly due to the jagged layout of the histogram, which results from binning the data (to create bars) and from the fact that the number of samples is large but not very large.\nFor the sampling distribution of means we know that the normal or (Student) t distribution represents the sampling distribution very accurately. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 2.\n\nThe borders demarcating the lowest and highest 2.5% of sample means in the theoretical probability distribution (the dotted lines) nicely coincide with the border between red or green and blue bars in the histogram in most of the sampling distributions that we generate with this app. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 3.\n\nThe width/peakedness of the sampling distribution changes but the normal curve fits the distribution well. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 4.\n\nA larger sample produces a sampling distribution that is more peaked. This means that the sample statistic outcomes are closer to the true population value (which is the mean of the sampling distribution).\nIn bags containing only two candies, we may often encounter bags without yellow candies (sample proportion of yellow candies: 0.0) or bags with two yellow candies (sample proportion of yellow candies: 1.0). Both values are quite different from the true population proportion (0.5).\nIn bags containing two-hundred candies, we will hardly ever encounter no yellow candies (0.0) or only yellow candies (1.0) if the proportion of candies in the population is 0.5. In these large bags, the proportion of yellow candies is usually be close to 0.5. The sample proportions are closer to the true population proportion. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 5.\n\nThe population proportion (parameter value) is equal to the average of the sampling distribution because the sample proportion is an unbiased estimator of the population proportion. So if we change the population proportion, the center of the sampling distribution changes accordingly.\nIn addition, the sampling distribution becomes less symmetrical/more skewed if the population proportion approaches zero or one. Because proportions cannot be less than zero or more than one, the sampling distribution cannot remain symmetrical if the population proportion is near zero or one. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 6.\n\nIt is in principle possible to draw a random sample of red candies separately from a random sample of yellow candies. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 7.\n\nIt is the difference between average weight of red candies and average weight of yellow candies in a sample.\nThis is illustrated by the equation directly above the graph of the sampling distribution, which subtracts the average weight of yellow candies (in yellow typeface) from the average weight of red candies (in red typeface). The result is added to the sampling distribution. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 8.\n\nThe sampling distribution has a bell shape like the normal or (Student) t distribution. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 9.\n\nThe true difference in averages in the population is what we expect as the average difference in the sampling distribution.\nAverage weight of red candies in the population is 2.8 grams and the average weight in the population of yellow candies is 3.1 grams. The average weight difference in the population is 2.8 - 3.1 = -0.3 grams. This is our expectation.\nThe centre of the sampling distribution is indeed at -0.3 if we draw thousands of samples. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 10.\n\nThe numbers on the horizontal axis in the sample histogram represent the difference in colour intensity for each pair of cases that is drawn.\nIn each draw, one case in the before population (red) and the same case in the after population (orange) is selected. The difference in colour intensity between the before and after measurement is calculated in the equation below the population dot plots. The calculated difference for this pair is represented by a dot in the figure in the middle. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 11.\n\nA case appears both in the before and after population: We have a before and after measurement of colour intensity for each case (candy). These two measurements are related or paired because they refer to the same candy.\nAs a consequence, if we draw candies for our before measurement, we also draw the candies for our after measurement. The after sample depends on the before sample. \n\n:::\n\n ::: {.cell type=‘rmdanswer’}\n\nAnswer to Question 12.\n\nThe numbers on the horizontal axis in the histogram of the sampling distribution signify the average difference in colour intensity of the candies in a sample (of five candies). \n\n:::"
  },
  {
    "objectID": "02-probability.html#spss-and-theoretical-approximation-of-the-sampling-distribution",
    "href": "02-probability.html#spss-and-theoretical-approximation-of-the-sampling-distribution",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.6 SPSS and Theoretical Approximation of the Sampling Distribution",
    "text": "2.6 SPSS and Theoretical Approximation of the Sampling Distribution\nBy default, SPSS uses a theoretical probability distribution to approximate the sampling distribution. It chooses the correct theoretical distribution but you yourself should check if the conditions for using this distribution are met. For example, is the sample large enough or is it plausible that the variable is normally distributed in the population?\nIn one case, SPSS automatically selects an exact approach if the conditions for a theoretical approximation are not met. If you apply a chi-squared test to a contingency table in SPSS, SPSS will automatically apply Fisher’s exact test if the table has two rows and two columns. In all other cases, you have to select bootstrapping or an exact approach yourself if the conditions for a theoretical approximation are not met.\nWe are not going to practice with theoretical approximations in SPSS, now. Because theoretical approximation is the default approach in SPSS, we will encounter it in the exercises in later chapters."
  },
  {
    "objectID": "02-probability.html#when-do-we-use-which-approach-to-the-sampling-distribution",
    "href": "02-probability.html#when-do-we-use-which-approach-to-the-sampling-distribution",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.7 When Do We Use Which Approach to the Sampling Distribution?",
    "text": "2.7 When Do We Use Which Approach to the Sampling Distribution?\n\n\n\n\n\nDiagram for selecting the type of sampling distribution.\n\n\n\n\nBy default, SPSS uses a theoretical approximation of the sampling distribution. Select the right test in SPSS and SPSS ensures that an appropriate theoretical probability distribution is used. You, however, must check whether the sample meets the conditions for using this theoretical probability distribution, see Table @ref(tab:thumb).\nIf the conditions for using a theoretical probability distribution are not met or if we do not have a theoretical approximation to the sampling distribution, we use bootstrapping or an exact approach. We can always use bootstrapping but an exact approach is available only if the variables are categorical. An exact approach is more accurate than bootstrapping and approximation with a theoretical probability distribution, for example, the chi-squared distribution, so we prefer the exact approach over bootstrapping if we are dealing with categorical variables."
  },
  {
    "objectID": "02-probability.html#test-your-understanding",
    "href": "02-probability.html#test-your-understanding",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.8 Test Your Understanding",
    "text": "2.8 Test Your Understanding\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhy does Figure @ref(fig:models-summary1) not show a population? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhich type of bootstrap sampling is better here: with or without replacement? Justify your answer. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nDraw a new initial sample in Figure @ref(fig:models-summary1). Is the bootstrapped sampling distribution going to resemble the true sampling distribution? Note that twenty per cent of the candies in the population are yellow. Motivate your answer. Draw 1,000 bootstrap samples to check your answer. \n\n:::\n\n\n\n\n\nNumber of heads for a toss of three coins.\n \n  \n    Number of heads \n    Combination \n  \n \n\n  \n    0 \n    tail-tail-tail \n  \n  \n    1 \n    tail-tail-head \n  \n  \n    1 \n    tail-head-tail \n  \n  \n    1 \n    head-tail-tail \n  \n  \n    2 \n    head-head-tail \n  \n  \n    2 \n    head-tail-head \n  \n  \n    2 \n    tail-head-head \n  \n  \n    3 \n    head-head-head \n  \n  \n    Total \n    8 \n  \n\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nCalculate the exact probability distribution of the number of heads in a toss of three fair coins (Table @ref(tab:models-summary-3)). \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nIn which situations can we use exact probabilities as a sampling distribution? \n\n:::\n\n\n\n\n\n ::: {.cell type=‘rmdquestion’}\n\n\nGenerate a sampling distribution of average sample candy weight in Figure @ref(fig:models-summary2). Try to explain in your own words why the sampling distribution of a sample mean has a bell shape. \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nWhich part of the graph in Figure @ref(fig:models-summary2) represents the theoretical probability distribution and what is the name of this distribution? \n\n:::\n\n ::: {.cell type=‘rmdquestion’}\n\n\nDoes this theoretical probability distribution always fit the simulated sampling distribution in Figure @ref(fig:models-summary2)? Create several sampling distributions and explain why we pay special attention to the lowest (red) and highest (green) 2.5% of the sample means. \n\n:::\n\n\nAnswers\n\n\n\nAnswers to the Test Your Understanding questions will be shown in the web book when the last tutor group has discussed this chapter.\n\n\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::\n ::: {.cell type=‘rmdanswer’}\n:::"
  },
  {
    "objectID": "02-probability.html#take-home-points",
    "href": "02-probability.html#take-home-points",
    "title": "2  Probability Models: How Do I Get a Sampling Distribution?",
    "section": "2.9 Take-Home Points",
    "text": "2.9 Take-Home Points\n\nWe may create an exact sampling distribution or simulate a bootstrap sampling distribution in simple situations or if we have a lot of computing power.\nFor a bootstrap sampling distribution, we need about 5,000 bootstrap samples from our original sample.\nAn exact sampling distribution can only be used with categorical variables.\nWe can often approximate the sampling distribution of a sample statistic with a known theoretical probability distribution.\nApproximations only work well under conditions, which we have to check.\nConditions usually involve the size of the sample, sample type (independent vs. dependent/paired), and the shape or variance of the population distribution.\nIf these conditions are not met or we do not have a theoretical approximation to the sampling distribution, we use bootstrapping or exact tests.\nSamples are independent if, in principle, we can draw a sample for one group without taking into account the sample for another group of cases. Otherwise, the samples are dependent or paired."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  }
]